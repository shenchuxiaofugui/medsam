{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "join = os.path.join\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import nibabel as nib\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从h5到h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步：拆分h5文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = \"/data3/home/lishengyong/data/ssc_3d/slices\"\n",
    "h5_path = \"h5所在的文件夹\"\n",
    "\n",
    "for i in [\"3D_images\", \"3D_masks\", \"jsons\"]:\n",
    "    os.makedirs(os.path.join(store_path, i), exist_ok=True)\n",
    "    \n",
    "def write(a, path, spacing=None):\n",
    "    b = sitk.GetImageFromArray(a)\n",
    "    if spacing is not None:\n",
    "        spacing = spacing.tolist()\n",
    "        b.SetSpacing(spacing=spacing)\n",
    "    sitk.WriteImage(b, path)\n",
    "    \n",
    "def section(store_path, h5path):\n",
    "    names = []    \n",
    "    with h5py.File(h5path, \"r\") as file:\n",
    "        for j in range(len(file[\"centers\"])):\n",
    "            name = f\"{file['names'][j][0].decode('utf-8')}_{file['centers'][j]}\"\n",
    "            names.append(name)\n",
    "            single_store = store_path + f\"/3D_images/{name}.nii.gz\"\n",
    "            data_3D = file[\"datas\"][j, 32:160, 32:160, :]\n",
    "            Mask_3D = file[\"masks\"][j, 32:160, 32:160, :]\n",
    "            Mask_3D[Mask_3D != 1] = 0\n",
    "            write(Mask_3D, single_store.replace(\"3D_images\", \"3D_masks\"), file[\"spacings\"][j])\n",
    "            write(data_3D, single_store, file[\"spacings\"][j])\n",
    "    with open(join(store_path, \"jsons\", h5path.name[:-2]+\"json\")) as json_file:\n",
    "        json.dump(names, json_file)\n",
    "            \n",
    "for i in Path(h5_path).glob(\"*.h5\"):\n",
    "    section(store_path, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步：推理\n",
    "可以直接用 infer_3d_multibox.py 得到每个轴的结果  \n",
    "运行方法：CUDA_VISIBLE_DEVICES=0 python infer_3d_multibox.py --axis {} --split {} --root {} --task_name{}  \n",
    "axis是表示从哪个轴推理，0轴是XY平面  \n",
    "split 默认为-1,表示不再拆分数据集，大概会占用3G多的显存，不然是0~4，表示数据集拆成五分，对应第几份，五份同时跑差不多吧72GCPU占满  \n",
    "root就是上面的store_path，task_name就是h5文件名（不包括.h5后缀）  \n",
    "\n",
    "infer_3d_multibox.py 得到第一轴的结果后，可以运行 iter_infer.py   \n",
    "CUDA_VISIBLE_DEVICES=0 python iter_infer.py --min_iter {} --max_iter {} --split {} --root {} --task_name{}  \n",
    "从上述1轴的结果产生2轴的结果，表示1个iter，如果是1（已有）--2--0顺序，那min_iter=2,max_iter=4  \n",
    "如果是1--2--0--1--2--0顺序，那min_iter=2,max_iter=7，确保iter % 3 是对应的轴  \n",
    "其余跟上面一样  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 产生的embadding文件如果之后用不到可以用\n",
    "find root_path -type d -name \"embadding\" -exec rm -r {} \\;清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：后处理\n",
    "### open操作(先腐蚀后膨胀去除小像素点)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import opening\n",
    "open_store_path = \"保存路径\"\n",
    "os.makedirs(open_store_path, exist_ok=True)\n",
    "opening(\"推理结果的路径，比如某个iter文件夹下面的infer文件夹\", open_store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迭代后的结果加回原来的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data3/home/lishengyong/data/ssc_3d/slices/jsons/lndb_weight.json\", \"r\") as file:\n",
    "    cases = json.load(file)\n",
    "\n",
    "direct_path = \"/data3/home/lishengyong/data/ssc_3d/slices/infer_3d_iter/axis_0/0818_98/multi_box\"\n",
    "iter_path = \"/data3/home/lishengyong/data/ssc_3d/slices/infer_3d_iter/axis_1/0818_98/iter9/opening\"\n",
    "store_path = \"/data3/home/lishengyong/data/ssc_3d/slices/infer_3d_iter/axis_0/0818_98/jiehe\"\n",
    "os.makedirs(store_path, exist_ok=True)\n",
    "\n",
    "for case in cases:   \n",
    "    case = case + \".nii.gz\"\n",
    "    direct_pred = nib.load(join(direct_path, case)).get_fdata() \n",
    "    direct_pred[direct_pred == 1] = 0\n",
    "    direct_pred[direct_pred == 2] = 1\n",
    "    iter_pred = nib.load(join(direct_path, case)).get_fdata() \n",
    "    has_mask = [index for index, arr in enumerate(direct_pred) if np.max(arr) != 0]\n",
    "    for i in has_mask:\n",
    "        iter_pred[i] = direct_pred[i]\n",
    "    nib.save(nib.Nifti1Image(iter_pred, None), join(store_path, case))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步 拼回H5文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1154/1154 [00:33<00:00, 34.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1154, 192, 192, 64)\n"
     ]
    }
   ],
   "source": [
    "pred_path = \"/data3/home/lishengyong/data/ssc_3d/slices/infer_3d_iter/axis_0/0818_98/jiehe\"\n",
    "save_path = \"/data3/home/lishengyong/data/ssc_3d/slices/infer_3d_iter/lndb_weigh.h5\"\n",
    "\n",
    "with open(\"/data3/home/lishengyong/data/ssc_3d/slices/jsons/lndb_weight.json\", \"r\") as file:\n",
    "    cases = json.load(file)\n",
    "print(len(cases))\n",
    "    \n",
    "preds = []\n",
    "for case in tqdm(cases, total=len(cases)):\n",
    "    case = case+\".nii.gz\"\n",
    "    pred = nib.load(join(pred_path, case))\n",
    "    pred_arr = pred.get_fdata()\n",
    "    pred_arr = np.transpose(pred_arr, (2,1,0))\n",
    "    # 根据情况来pad\n",
    "    pred_arr = np.pad(pred_arr, ((32,32), (32,32), (0,0)), mode=\"constant\", constant_values=0).astype(int)\n",
    "    if np.max(pred_arr) == 0:\n",
    "        print(case)\n",
    "    preds.append(pred_arr)\n",
    "\n",
    "pred_arr = np.stack(preds)\n",
    "print(pred_arr.shape)\n",
    "with h5py.File(save_path, \"w\") as file:\n",
    "    file[\"masks\"] = pred_arr\n",
    "    \n",
    "# pred_img = nib.Nifti1Image(pred_arr, None)\n",
    "# nib.save(pred_img, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "a = nib.load(\"/data3/home/lishengyong/data/ssc_3d/slices/infer_3d_iter/lndb_weigh.nii.gz\").get_fdata()\n",
    "save_path = \"/data3/home/lishengyong/data/ssc_3d/slices/infer_3d_iter/lndb_weigh.h5\"\n",
    "a = a.astype(bool)\n",
    "print(a[1,1,...])\n",
    "with h5py.File(save_path, \"w\") as file:\n",
    "    file[\"masks\"] = a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
